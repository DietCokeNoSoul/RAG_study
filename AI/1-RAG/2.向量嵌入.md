---
tags:
  - 代码
---
## 向量嵌入

使用类似于Transformer的Encoder来对句子进行编码。

嵌入之后，不受语言、字数限制。

## 向量相似度 

1. **余弦相似度**，取值\[-1,1]，关注方向。
	分子的点积同时包含长度和方向信息，分母是长度信息，分子除分母消除长度信息，剩下方向。
	现有的文本向量化模型，多多少少都是直接采用余弦相似性或者间接让两向量的角度尽可能小来做损失函数的，这就是为什么计算余弦相似度**对文本向量有效**，不管文本多长，只要主题相同，相似度就高。
	
$$\cos(\theta) = \frac{\vec{A} \cdot \vec{B}}{|\vec{A}||\vec{B}|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}$$
为什么长度信息无效？证明：
设两个向量 $\vec{A}$ 和 $\vec{B}$，余弦相似度公式为：
$$\cos(\theta) = \frac{\vec{A} \cdot \vec{B}}{|\vec{A}||\vec{B}|}$$
现在考虑 $\vec{A}$ 缩放 $k$ 倍：$\vec{A'} = k\vec{A}$（其中 $k > 0$）
$$\cos(\theta') = \frac{(k\vec{A}) \cdot \vec{B}}{|k\vec{A}||\vec{B}|}$$
$$= \frac{k(\vec{A} \cdot \vec{B})}{k|\vec{A}||\vec{B}|}$$
$$= \frac{k(\vec{A} \cdot \vec{B})}{k|\vec{A}||\vec{B}|} = \frac{\vec{A} \cdot \vec{B}}{|\vec{A}||\vec{B}|}$$
**结论：** $$\cos(\theta') = \cos(\theta)$$
```python
# 调库
from sklearn.metrics.pairwise import cosine_similarity
def cosine_sim(vec1, vec2):
    return cosine_similarity([vec1], [vec2])[0][0]


# 手动实现
def cosine_similarity_manual(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_a = np.linalg.norm(vec1) # 二范数
    norm_b = np.linalg.norm(vec2)
    return dot_product / (norm_a * norm_b)
```

2. 欧几里得距离：向量的欧氏距离，可能收到维度影响大
$$d_{euclidean}(\vec{A}, \vec{B}) = \sqrt{\sum_{i=1}^{n} (A_i - B_i)^2}$$
```python
def euclidean_distance(vec1, vec2):
    return np.linalg.norm(vec1 - vec2)
  
# 转换为相似度（距离越小，相似度越高）取值(0,1]
def euclidean_similarity(vec1, vec2):
    distance = euclidean_distance(vec1, vec2)
    return 1 / (1 + distance)
```

3. 曼哈顿距离：
$$d_{manhattan}(\vec{A}, \vec{B}) = \sum_{i=1}^{n} |A_i - B_i|$$
```python
def manhattan_distance(vec1, vec2):
    return np.sum(np.abs(vec1 - vec2))
```

4. 点积相似度：
$$sim_{dot}(\vec{A}, \vec{B}) = \vec{A} \cdot \vec{B} = \sum_{i=1}^{n} A_i B_i$$
```python
def dot_product_similarity(vec1, vec2):
    return np.dot(vec1, vec2)
```

5. Jaccard相似度：适合二进制向量
$$J(\vec{A}, \vec{B}) = \frac{|\vec{A} \cap \vec{B}|}{|\vec{A} \cup \vec{B}|} = \frac{\sum_{i=1}^{n} (A_i \land B_i)}{\sum_{i=1}^{n} (A_i \lor B_i)}$$
```python
def jaccard_similarity(vec1, vec2):
    intersection = np.sum(np.logical_and(vec1, vec2))
    union = np.sum(np.logical_or(vec1, vec2))
    return intersection / union
```


## 利用Embedding模型实操

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B')
sentence = [
    "我是你女朋友!",
    "牵我的手！",
    "天气怎么样？"
]
  
embeddings = model.encode(sentence, convert_to_tensor=True, truncate_dim=512)
similarity = model.similarity(embeddings,embeddings)
```