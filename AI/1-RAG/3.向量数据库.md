---
tags:
  - 操作
---
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B')
sentence = [
    "我是你女朋友!",
    "牵我的手！",
    "天气怎么样？"
]

embeddings = model.encode(sentence, convert_to_tensor=True, truncate_dim=512)
similarity = model.similarity(embeddings,embeddings)
```
把上述代码生成的向量存起来，用于以后匹配用户输入，因此需要用到向量数据库。

向量数据库实际存储格式：

| ID  |          向量           |    原文本    |  其他元数据  |
| :-: | :-------------------: | :-------: | :-----: |
|  1  | [0.1,0.2,0.3,...,0.8] | "我是你女朋友!" | 时间戳、分类等 |
|  2  | [0.4,0.1,0.7,...,0.2] |  "牵我的手！"  |         |
|  3  | [0.6,0.9,0.1,...,0.5] | "天气怎么样？"  |         |

**暴力搜索问题：**

- 如果有100万条向量，每次查询需要计算100万次相似度，速度太慢！

**解决方案 - 近似最近邻搜索ANN：**
牺牲一点点精度来换取数量级的速度提升，同时为了补救精度，一般返回Top k个候选检索结果。

## 近似最近邻搜索ANN

 ##### **1. 基于树的方法：**
Annoy (Approximate Nearest Neighbors Oh Yeah)
- 构建随机投影树
- 每个节点用随机超平面分割空间
- 查询时遍历多棵树取交集

KD-Tree
- 传统方法，在低维空间有效
- 高维空间性能下降（维度诅咒）

 ##### **2. 基于图的方法：**
HNSW (Hierarchical Navigable Small World)
- 最流行的方法之一
- 构建多层图，每层是小世界网络
- 查询从顶层开始，逐层细化搜索
- Pinecone、Weaviate等都使用

NSW (Navigable Small World)
- HNSW的前身，单层版本

 ##### **3. 基于聚类的方法：**
IVF (Inverted File)
- 用K-means将空间分成多个聚类
- 查询时只搜索最近的几个聚类中心
- 适合大规模数据

Product Quantization (PQ)
- 向量量化压缩，减少内存占用
- 与IVF结合形成IVFPQ

 #####  **4. 基于哈希的方法：**
LSH (Locality-Sensitive Hashing)
- 相似向量映射到相同哈希桶
- 多个哈希函数提高准确率

Random Projection
- 随机投影降维
- Johnson-Lindenstrauss定理保证距离保持

 #####  **5. 基于学习的方法：**
Learned Index
- 用神经网络学习数据分布
- 预测向量在排序数组中的位置

 ##### **性能对比：**
- **HNSW**: 查询速度快，内存占用大
- **IVF**: 内存友好，适合大规模
- **Annoy**: 构建快，查询中等
- **LSH**: 理论简单，实际效果一般

**大多数向量数据库都使用HNSW作为默认索引方法。**


## 主流向量数据库

#### ChromaDB
开源，API友好，易上手，支持本地持久化存储。