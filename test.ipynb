{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6309e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f8f89",
   "metadata": {},
   "source": [
    "## PDF切块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a94987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_from_directory(pdf_directory):\n",
    "    \"\"\"处理目录中的所有PDF文件（传统方式）\"\"\"\n",
    "    \n",
    "    # 加载PDF文件\n",
    "    loader = DirectoryLoader(\n",
    "        pdf_directory, \n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"处理了 {len(documents)} 个PDF文档\")\n",
    "    \n",
    "    # 文本切割\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # 每块大小\n",
    "        chunk_overlap=200,  # 重叠部分\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # 分割符优先级\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"切割成 {len(chunks)} 个文本块\")\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f1ce7",
   "metadata": {},
   "source": [
    "## 向量化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e65ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始加载向量库...\n",
      "CUDA 是否可用: True\n",
      "GPU 数量: 1\n",
      "当前 GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "首次加载嵌入模型...\n",
      "✅ 嵌入模型加载完成 (设备: cuda:0)，耗时: 18.02秒\n",
      "发现已存在的向量库，直接加载...\n",
      "✅ 向量库加载完成，耗时: 0.12秒\n",
      "总耗时: 19.73秒\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "print(\"\\n开始加载向量库...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 检查设备可用性\n",
    "print(f\"CUDA 是否可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"当前 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    print(\"未检测到 CUDA，将使用 CPU\")\n",
    "    device = 'cpu'\n",
    "\n",
    "persist_dir = \"./chroma_db\"\n",
    "collection_name = \"pdf_collection\"\n",
    "\n",
    "# 全局模型缓存：只在首次运行时加载模型\n",
    "if 'embedding' not in globals():\n",
    "    print(\"首次加载嵌入模型...\")\n",
    "    model_start = time.time()\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name='Qwen/Qwen3-Embedding-0.6B',\n",
    "        cache_folder='Models',\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    print(f\"✅ 嵌入模型加载完成 (设备: {device})，耗时: {time.time() - model_start:.2f}秒\")\n",
    "else:\n",
    "    print(\"使用已缓存的嵌入模型 ✓\")\n",
    "\n",
    "# 检查向量库是否存在\n",
    "if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "    print(\"发现已存在的向量库，直接加载...\")\n",
    "    load_start = time.time()\n",
    "    \n",
    "    vector_store = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embedding,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 向量库加载完成，耗时: {time.time() - load_start:.2f}秒\")\n",
    "    print(f\"总耗时: {time.time() - start_time:.2f}秒\")\n",
    "else:\n",
    "    print(\"向量库不存在，需要重新创建...\")\n",
    "    pdf_chunks = process_pdfs_from_directory(\"Paper/2ATAKKQD\")\n",
    "    vector_store = Chroma.from_documents(\n",
    "        pdf_chunks,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=collection_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd9966",
   "metadata": {},
   "source": [
    "## 检索器设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8363a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfc324",
   "metadata": {},
   "source": [
    "## prompt模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b44acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful AI assistant. Use the following pieces of context to answer the question at the end, and you need to show the source of your answer.\n",
    "{context}\n",
    "If you don't know the answer, just say you don't know. Don't try to make up an answer.\n",
    "Question: {question}\n",
    "Answer in English.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35310e",
   "metadata": {},
   "source": [
    "## 大语言模型设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9978d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "api_key = os.environ.get(\"QWEN_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"QWEN_API_KEY environment variable is not set\")\n",
    "\n",
    "# 使用更新的参数名称避免警告\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen3-next-80b-a3b-instruct\",\n",
    "    api_key=api_key,  # 使用api_key而不是openai_api_key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 使用base_url而不是openai_api_base\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# deepseek-chat 示例代码 --- IGNORE ---\n",
    "# api_key = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# if not api_key:\n",
    "#     raise ValueError(\"DEEPSEEK_API_KEY environment variable is not set\")\n",
    "\n",
    "# # 使用更新的参数名称避免警告\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"deepseek-chat\",\n",
    "#     api_key=api_key,  # 使用api_key而不是openai_api_key\n",
    "#     base_url=\"https://api.deepseek.com/v1\",  # 使用base_url而不是openai_api_base\n",
    "#     temperature=0.1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809198c",
   "metadata": {},
   "source": [
    "## 检索增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccf40c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\71949\\AppData\\Local\\Temp\\ipykernel_1916\\117658635.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = base_retriever.get_relevant_documents(\"What is the relationship between microbes and drugs?\")\n",
      "d:\\app\\anaconda3\\envs\\use-pytorch\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:83: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 3 个相关文档\n",
      "提示词: \n",
      "You are a helpful AI assistant. Use the following pieces of context to answer the question at the end, and you need to show the source of your answer.\n",
      "∗T o whom correspondence should be addressed.\n",
      "Associate Editor: XXXXXXX\n",
      "Received on XXXXX; revised on XXXXX; accepted on XXXXX\n",
      "Abstract\n",
      "Motivation: The microbes in human body play a crucial role in influencing the functions of drugs, as they can regulate the activities and\n",
      "toxicities of drugs. Most recent methods for predicting drug-microbe associations are based on graph learning. However, the relationships\n",
      "among multiple drugs and microbes are complex, diverse and heterogeneous. Existing methods often fail to fully model the relationships.\n",
      "In addition, the attributes of drug-microbe pairs exhibit long-distance spatial correlations, which previous methods have not integrated\n",
      "effectively.\n",
      "Results: We propose a new prediction method named DHDMP which is designed to encode the relationships among multiple drugs\n",
      "and microbes and integrate the attributes of various neighbor nodes along with the pairwise long-distance correlations. First, we\n",
      "∗T o whom correspondence should be addressed.\n",
      "Associate Editor: XXXXXXX\n",
      "Received on XXXXX; revised on XXXXX; accepted on XXXXX\n",
      "Abstract\n",
      "Motivation: The microbes in human body play a crucial role in influencing the functions of drugs, as they can regulate the activities and\n",
      "toxicities of drugs. Most recent methods for predicting drug-microbe associations are based on graph learning. However, the relationships\n",
      "among multiple drugs and microbes are complex, diverse and heterogeneous. Existing methods often fail to fully model the relationships.\n",
      "In addition, the attributes of drug-microbe pairs exhibit long-distance spatial correlations, which previous methods have not integrated\n",
      "effectively.\n",
      "Results: We propose a new prediction method named DHDMP which is designed to encode the relationships among multiple drugs\n",
      "and microbes and integrate the attributes of various neighbor nodes along with the pairwise long-distance correlations. First, we\n",
      "bioinformatics , 23(1):492, 2022.\n",
      "Tian, Z., Yu, Y ., Fang, H., Xie, W., and Guo, M. Predicting microbe–\n",
      "drug associations with structure-enhanced contrastive learning and self-paced\n",
      "negative sampling strategy. Briefings in Bioinformatics , 24(2):bbac634, 2023.\n",
      "If you don't know the answer, just say you don't know. Don't try to make up an answer.\n",
      "Question: What is the relationship between microbes and drugs?\n",
      "Answer in English.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = base_retriever.get_relevant_documents(\"What is the relationship between microbes and drugs?\")\n",
    "print(f\"找到 {len(relevant_docs)} 个相关文档\")\n",
    "\n",
    "prompt_text = prompt.format(\n",
    "    context=\"\\n\".join([doc.page_content for doc in relevant_docs]),\n",
    "    question=\"What is the relationship between microbes and drugs?\"\n",
    ")\n",
    "print(\"提示词:\", prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba6feb",
   "metadata": {},
   "source": [
    "## 生成回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6717601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\71949\\AppData\\Local\\Temp\\ipykernel_1916\\1665534791.py:4: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([HumanMessage(content=prompt_text)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回答: The microbes in the human body play a crucial role in influencing the functions of drugs, as they can regulate the activities and toxicities of drugs. \n",
      "\n",
      "Source: Abstract from the provided context.\n"
     ]
    }
   ],
   "source": [
    "# 使用现代的LangChain调用方式\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = llm([HumanMessage(content=prompt_text)])\n",
    "print(\"回答:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8391d2c",
   "metadata": {},
   "source": [
    "## RAGAs评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d182d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_input', 'response', 'retrieved_contexts', 'reference'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "{\n",
    "    \"user_input\":[], <-- 问题基于Context\n",
    "    \"response\":[], <-- 答案基于LLM生成\n",
    "    \"retrieved_contexts\":[], <-- 检索到的上下文（改名）\n",
    "    \"reference\":[] <-- 标准答案（改名）\n",
    "}\n",
    "'''\n",
    "from datasets import Dataset\n",
    "\n",
    "user_input = [\n",
    "    \"What is the relationship between microbes and drugs?\"\n",
    "]\n",
    "response_list = [\n",
    "    response.content  # 使用ChatOpenAI的response.content\n",
    "]\n",
    "retrieved_contexts = [\n",
    "    [doc.page_content for doc in relevant_docs]  # 改为列表格式\n",
    "]\n",
    "reference = [\n",
    "    \"Microbes can influence the metabolism of drugs, affecting their efficacy and toxicity. They can activate, inactivate, or modify drugs through various biochemical processes. This interaction can lead to variations in drug response among individuals based on their unique microbiome composition.\"\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"user_input\": user_input,\n",
    "    \"response\": response_list,\n",
    "    \"retrieved_contexts\": retrieved_contexts,\n",
    "    \"reference\": reference\n",
    "}\n",
    "\n",
    "dataset_ragas = Dataset.from_dict(data)\n",
    "dataset_ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7eeedc",
   "metadata": {},
   "source": [
    "## RAGAs评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86b74a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 使用全局缓存的embedding模型...\n",
      "模型: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\71949\\AppData\\Local\\Temp\\ipykernel_8496\\803990485.py:20: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluate_embedding = LangchainEmbeddingsWrapper(embedding)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2c44c5084b4a6097334e9a8e159a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset_ragas)\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy\n",
    ")\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# 使用已经全局缓存的embedding模型\n",
    "print(\"🔄 使用全局缓存的embedding模型...\")\n",
    "print(f\"模型: {embedding.model_name}\")\n",
    "\n",
    "# 将LangChain的embedding包装为RAGAs可用的格式\n",
    "evaluate_embedding = LangchainEmbeddingsWrapper(embedding)\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# 使用RAGAs内置的嵌入模型\n",
    "result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), LLMContextPrecisionWithReference(),AnswerRelevancy()],llm=evaluator_llm,embeddings=evaluate_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d0af98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>llm_context_precision_with_reference</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the relationship between microbes and ...</td>\n",
       "      <td>[∗T o whom correspondence should be addressed....</td>\n",
       "      <td>Hello! 😊  \\nI'm Qwen, a large-scale language m...</td>\n",
       "      <td>Microbes can influence the metabolism of drugs...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.323818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is the relationship between microbes and ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [∗T o whom correspondence should be addressed....   \n",
       "\n",
       "                                            response  \\\n",
       "0  Hello! 😊  \\nI'm Qwen, a large-scale language m...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  Microbes can influence the metabolism of drugs...        0.333333   \n",
       "\n",
       "   faithfulness  llm_context_precision_with_reference  answer_relevancy  \n",
       "0           NaN                                   1.0          0.323818  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "use-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
