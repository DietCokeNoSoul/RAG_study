{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6309e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f8f89",
   "metadata": {},
   "source": [
    "## PDFåˆ‡å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a94987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_from_directory(pdf_directory):\n",
    "    \"\"\"å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰\"\"\"\n",
    "    \n",
    "    # åŠ è½½PDFæ–‡ä»¶\n",
    "    loader = DirectoryLoader(\n",
    "        pdf_directory, \n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"å¤„ç†äº† {len(documents)} ä¸ªPDFæ–‡æ¡£\")\n",
    "    \n",
    "    # æ–‡æœ¬åˆ‡å‰²\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # æ¯å—å¤§å°\n",
    "        chunk_overlap=200,  # é‡å éƒ¨åˆ†\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # åˆ†å‰²ç¬¦ä¼˜å…ˆçº§\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"åˆ‡å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—\")\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f1ce7",
   "metadata": {},
   "source": [
    "## å‘é‡åŒ–å­˜å‚¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e65ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹åŠ è½½å‘é‡åº“...\n",
      "CUDA æ˜¯å¦å¯ç”¨: True\n",
      "GPU æ•°é‡: 1\n",
      "å½“å‰ GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "é¦–æ¬¡åŠ è½½åµŒå…¥æ¨¡å‹...\n",
      "âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: cuda:0)ï¼Œè€—æ—¶: 16.69ç§’\n",
      "å‘ç°å·²å­˜åœ¨çš„å‘é‡åº“ï¼Œç›´æ¥åŠ è½½...\n",
      "âœ… å‘é‡åº“åŠ è½½å®Œæˆï¼Œè€—æ—¶: 0.18ç§’\n",
      "æ€»è€—æ—¶: 18.52ç§’\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\" # ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ä¸‹è½½æ¨¡å‹\n",
    "\n",
    "print(\"\\nå¼€å§‹åŠ è½½å‘é‡åº“...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§\n",
    "print(f\"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"å½“å‰ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda:0'\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU\")\n",
    "    device = 'cpu'\n",
    "\n",
    "persist_dir = \"./chroma_db\"\n",
    "collection_name = \"pdf_collection\"\n",
    "\n",
    "# å…¨å±€æ¨¡å‹ç¼“å­˜ï¼šåªåœ¨é¦–æ¬¡è¿è¡Œæ—¶åŠ è½½æ¨¡å‹\n",
    "if 'embedding' not in globals():\n",
    "    print(\"é¦–æ¬¡åŠ è½½åµŒå…¥æ¨¡å‹...\")\n",
    "    model_start = time.time()\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings( # éœ€è¦å¼€vpn\n",
    "        model_name='Qwen/Qwen3-Embedding-0.6B',\n",
    "        cache_folder='Models',\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    print(f\"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})ï¼Œè€—æ—¶: {time.time() - model_start:.2f}ç§’\")\n",
    "else:\n",
    "    print(\"ä½¿ç”¨å·²ç¼“å­˜çš„åµŒå…¥æ¨¡å‹ âœ“\")\n",
    "\n",
    "# æ£€æŸ¥å‘é‡åº“æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "    print(\"å‘ç°å·²å­˜åœ¨çš„å‘é‡åº“ï¼Œç›´æ¥åŠ è½½...\")\n",
    "    load_start = time.time()\n",
    "    \n",
    "    vector_store = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embedding,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… å‘é‡åº“åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - load_start:.2f}ç§’\")\n",
    "    print(f\"æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’\")\n",
    "else:\n",
    "    print(\"å‘é‡åº“ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°åˆ›å»º...\")\n",
    "    pdf_chunks = process_pdfs_from_directory(\"Paper/2ATAKKQD\")\n",
    "    vector_store = Chroma.from_documents(\n",
    "        pdf_chunks,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=collection_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd9966",
   "metadata": {},
   "source": [
    "## æ£€ç´¢å™¨è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8363a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfc324",
   "metadata": {},
   "source": [
    "## promptæ¨¡ç‰ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b44acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful AI assistant. Use the following pieces of context to answer the question at the end, and you need to show the source of your answer.\n",
    "{context}\n",
    "If you don't know the answer, just say you don't know. Don't try to make up an answer.\n",
    "Question: {question}\n",
    "Answer in English.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35310e",
   "metadata": {},
   "source": [
    "## å¤§è¯­è¨€æ¨¡å‹è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9978d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "api_key = os.environ.get(\"QWEN_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"QWEN_API_KEY environment variable is not set\")\n",
    "\n",
    "# ä½¿ç”¨æ›´æ–°çš„å‚æ•°åç§°é¿å…è­¦å‘Š\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen3-next-80b-a3b-instruct\",\n",
    "    api_key=api_key,  # ä½¿ç”¨api_keyè€Œä¸æ˜¯openai_api_key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # ä½¿ç”¨base_urlè€Œä¸æ˜¯openai_api_base\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# deepseek-chat ç¤ºä¾‹ä»£ç  --- IGNORE ---\n",
    "# api_key = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# if not api_key:\n",
    "#     raise ValueError(\"DEEPSEEK_API_KEY environment variable is not set\")\n",
    "\n",
    "# # ä½¿ç”¨æ›´æ–°çš„å‚æ•°åç§°é¿å…è­¦å‘Š\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"deepseek-chat\",\n",
    "#     api_key=api_key,  # ä½¿ç”¨api_keyè€Œä¸æ˜¯openai_api_key\n",
    "#     base_url=\"https://api.deepseek.com/v1\",  # ä½¿ç”¨base_urlè€Œä¸æ˜¯openai_api_base\n",
    "#     temperature=0.1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809198c",
   "metadata": {},
   "source": [
    "## æ£€ç´¢å¢å¼º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccf40c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\71949\\AppData\\Local\\Temp\\ipykernel_51548\\117658635.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = base_retriever.get_relevant_documents(\"What is the relationship between microbes and drugs?\")\n",
      "d:\\app\\anaconda3\\envs\\use-pytorch\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:83: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰¾åˆ° 3 ä¸ªç›¸å…³æ–‡æ¡£\n",
      "æç¤ºè¯: \n",
      "You are a helpful AI assistant. Use the following pieces of context to answer the question at the end, and you need to show the source of your answer.\n",
      "âˆ—T o whom correspondence should be addressed.\n",
      "Associate Editor: XXXXXXX\n",
      "Received on XXXXX; revised on XXXXX; accepted on XXXXX\n",
      "Abstract\n",
      "Motivation: The microbes in human body play a crucial role in influencing the functions of drugs, as they can regulate the activities and\n",
      "toxicities of drugs. Most recent methods for predicting drug-microbe associations are based on graph learning. However, the relationships\n",
      "among multiple drugs and microbes are complex, diverse and heterogeneous. Existing methods often fail to fully model the relationships.\n",
      "In addition, the attributes of drug-microbe pairs exhibit long-distance spatial correlations, which previous methods have not integrated\n",
      "effectively.\n",
      "Results: We propose a new prediction method named DHDMP which is designed to encode the relationships among multiple drugs\n",
      "and microbes and integrate the attributes of various neighbor nodes along with the pairwise long-distance correlations. First, we\n",
      "âˆ—T o whom correspondence should be addressed.\n",
      "Associate Editor: XXXXXXX\n",
      "Received on XXXXX; revised on XXXXX; accepted on XXXXX\n",
      "Abstract\n",
      "Motivation: The microbes in human body play a crucial role in influencing the functions of drugs, as they can regulate the activities and\n",
      "toxicities of drugs. Most recent methods for predicting drug-microbe associations are based on graph learning. However, the relationships\n",
      "among multiple drugs and microbes are complex, diverse and heterogeneous. Existing methods often fail to fully model the relationships.\n",
      "In addition, the attributes of drug-microbe pairs exhibit long-distance spatial correlations, which previous methods have not integrated\n",
      "effectively.\n",
      "Results: We propose a new prediction method named DHDMP which is designed to encode the relationships among multiple drugs\n",
      "and microbes and integrate the attributes of various neighbor nodes along with the pairwise long-distance correlations. First, we\n",
      "bioinformatics , 23(1):492, 2022.\n",
      "Tian, Z., Yu, Y ., Fang, H., Xie, W., and Guo, M. Predicting microbeâ€“\n",
      "drug associations with structure-enhanced contrastive learning and self-paced\n",
      "negative sampling strategy. Briefings in Bioinformatics , 24(2):bbac634, 2023.\n",
      "If you don't know the answer, just say you don't know. Don't try to make up an answer.\n",
      "Question: What is the relationship between microbes and drugs?\n",
      "Answer in English.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = base_retriever.get_relevant_documents(\"What is the relationship between microbes and drugs?\")\n",
    "print(f\"æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "\n",
    "prompt_text = prompt.format(\n",
    "    context=\"\\n\".join([doc.page_content for doc in relevant_docs]),\n",
    "    question=\"What is the relationship between microbes and drugs?\"\n",
    ")\n",
    "print(\"æç¤ºè¯:\", prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba6feb",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆå›ç­”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6717601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\71949\\AppData\\Local\\Temp\\ipykernel_51548\\1665534791.py:4: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([HumanMessage(content=prompt_text)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›ç­”: The relationship between microbes and drugs is that microbes in the human body play a crucial role in influencing the functions of drugs, as they can regulate the activities and toxicities of drugs. \n",
      "\n",
      "Source: Abstract from the provided context.\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ç°ä»£çš„LangChainè°ƒç”¨æ–¹å¼\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = llm([HumanMessage(content=prompt_text)])\n",
    "print(\"å›ç­”:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8391d2c",
   "metadata": {},
   "source": [
    "## RAGAsè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d182d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_input', 'response', 'retrieved_contexts', 'reference'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "{\n",
    "    \"user_input\":[], <-- é—®é¢˜åŸºäºContext\n",
    "    \"response\":[], <-- ç­”æ¡ˆåŸºäºLLMç”Ÿæˆ\n",
    "    \"retrieved_contexts\":[], <-- æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆæ”¹åï¼‰\n",
    "    \"reference\":[] <-- æ ‡å‡†ç­”æ¡ˆï¼ˆæ”¹åï¼‰\n",
    "}\n",
    "'''\n",
    "from datasets import Dataset\n",
    "\n",
    "user_input = [\n",
    "    \"What is the relationship between microbes and drugs?\"\n",
    "]\n",
    "response_list = [\n",
    "    response.content  # ä½¿ç”¨ChatOpenAIçš„response.content\n",
    "]\n",
    "retrieved_contexts = [\n",
    "    [doc.page_content for doc in relevant_docs]  # æ”¹ä¸ºåˆ—è¡¨æ ¼å¼\n",
    "]\n",
    "reference = [\n",
    "    \"Microbes can influence the metabolism of drugs, affecting their efficacy and toxicity. They can activate, inactivate, or modify drugs through various biochemical processes. This interaction can lead to variations in drug response among individuals based on their unique microbiome composition.\"\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"user_input\": user_input,\n",
    "    \"response\": response_list,\n",
    "    \"retrieved_contexts\": retrieved_contexts,\n",
    "    \"reference\": reference\n",
    "}\n",
    "\n",
    "dataset_ragas = Dataset.from_dict(data)\n",
    "dataset_ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7eeedc",
   "metadata": {},
   "source": [
    "## RAGAsè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b74a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ä½¿ç”¨å…¨å±€ç¼“å­˜çš„embeddingæ¨¡å‹...\n",
      "æ¨¡å‹: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\71949\\AppData\\Local\\Temp\\ipykernel_51548\\803990485.py:20: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluate_embedding = LangchainEmbeddingsWrapper(embedding)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d70b9982ef43b39eeca5b6dfb964ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset_ragas)\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy\n",
    ")\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# ä½¿ç”¨å·²ç»å…¨å±€ç¼“å­˜çš„embeddingæ¨¡å‹\n",
    "print(\"ğŸ”„ ä½¿ç”¨å…¨å±€ç¼“å­˜çš„embeddingæ¨¡å‹...\")\n",
    "print(f\"æ¨¡å‹: {embedding.model_name}\")\n",
    "\n",
    "# å°†LangChainçš„embeddingåŒ…è£…ä¸ºRAGAså¯ç”¨çš„æ ¼å¼\n",
    "evaluate_embedding = LangchainEmbeddingsWrapper(embedding)\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# ä½¿ç”¨RAGAså†…ç½®çš„åµŒå…¥æ¨¡å‹\n",
    "result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), LLMContextPrecisionWithReference(),AnswerRelevancy()],llm=evaluator_llm,embeddings=evaluate_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0af98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>llm_context_precision_with_reference</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the relationship between microbes and ...</td>\n",
       "      <td>[âˆ—T o whom correspondence should be addressed....</td>\n",
       "      <td>The relationship between microbes and drugs is...</td>\n",
       "      <td>Microbes can influence the metabolism of drugs...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.87999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is the relationship between microbes and ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [âˆ—T o whom correspondence should be addressed....   \n",
       "\n",
       "                                            response  \\\n",
       "0  The relationship between microbes and drugs is...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  Microbes can influence the metabolism of drugs...        0.333333   \n",
       "\n",
       "   faithfulness  llm_context_precision_with_reference  answer_relevancy  \n",
       "0           1.0                                   1.0           0.87999  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "use-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
